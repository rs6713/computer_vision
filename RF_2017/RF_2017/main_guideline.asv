% Simple Random Forest Toolbox for Matlab
% written by Mang Shao and Tae-Kyun Kim, June 20, 2014.
% updated by Tae-Kyun Kim, Feb 09, 2017

% This is a guideline script of simple-RF toolbox.
% The codes are made for educational purposes only.
% Some parts are inspired by Karpathy's RF Toolbox

% Under BSD Licence

% Initialisation
init;

% Select dataset
[data_train, data_test] = getData('Toy_Spiral'); % {'Toy_Gaussian', 'Toy_Spiral', 'Toy_Circle', 'Caltech'}


%%%%%%%%%%%%%
% check the training and testing data
    % data_train(:,1:2) : [num_data x dim] Training 2D vectors
    % data_train(:,3) : [num_data x 1] Labels of training data, {1,2,3}


%4 training bags, size half of original dataset
bag_size=uint8(size(data_train,1)/2);
training_bags=zeros(4,bag_size, 3);

for bag= 1:4
    rand_select=randperm(size(data_train,1));
    training_bags(bag,:,:)= data_train(rand_select(1, 1:bag_size),:);
end
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%TODO
%Try diff bagging techniques
%Try diff splitting functions (singular dimension, more random, more
%diverse trees, weaker, less correlation)
%How join trees leaf prob distrs, product, avg? how robust to
%noise?majority voting?
%construction/prediction depends Model params: dept, randomness 
%(controlled by p)and its type, type. choice weak learner model, training
%obj function. Affect confidence, efficiency(COMPARE TIMES),  accuracy
%deep trees can ->overfitting. Test accuracy increase with forest size.
%how randomness affects tree correlation & generalisation
%choice of stopiing criteria, influence tree shape, whether balanced, (want
%to avoid un), at limit may become chain of weak learners, little feature
%sharing, thus little generalisation.
%LOOK AT NOTES SUMMARY FOR INTRO/CONCLUSION
%Curremtly use axis aligned weak learner, pick feature randomly, pick rand
%val in range as threshold of chosen feature, repeat the splitNum times,
%choose best split. DISPLAY SPLITS & CHOICE?
%Use fixed depth, emptiness as stop criteria.
%show 4 test points on graph with orig swirl with colours?
%no trees (1,3,5,10,20)with 5 3 'IG'
%DEPTH [2,5,7,11]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    % data_test(:,1:2) : [num_data x dim] Testing 2D vectors, 2D points in the
    % uniform dense grid within the range of [-1.5, 1.5]
    % data_train(:,3) : N/A
    
scatter(data_test(:,1),data_test(:,2),'.b');


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Que 1 part 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%{
% Set the random forest parameters for instance, 
param.num = 1;         % Number of trees 10
param.depth = 2;        % trees depth 5 
param.splitNum = 3;     % Number of split functions to try
param.split = 'IG';     % Currently support 'information gain' only

%%%%%%%%%%%%%%%%%%%%%%
% Train Random Forest 

%tree = growTrees(training_bags(1,:, :),param);

% Grow all trees
trees = growTrees(data_train,param);
%l_node=trees(1).node(2);
%r_node=trees(1).node(3);
%l_node_data=data_train(l_node.idx.', :);
%fig =visualise(l_node_data,l_node.prob,[],false) %doesnt work as l_node.prob is not 3D
%get(fig)
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Que 1 - part b
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%{
param.num = 1;         % Number of trees 10
param.depth = 10;        % trees depth 5 
param.splitNum = 3;     % Number of split functions to try
param.split = 'IG';     % Currently support 'information gain' only
tree = growTrees(data_train,param);

%9 example leaf renders
rand_leafs=randperm(size(tree.leaf,2));
no_div=9;
for L= 1:no_div %length(tree.leaf)
    subplot(3,3,L);
    bar(tree(1).leaf(rand_leafs(L)).prob);
    axis([0.5 3.5 0 1]);
end
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Que 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
param.num = 10;         % Number of trees 10
param.depth = 5;        % trees depth 5 
param.splitNum = 5;     % Number of split functions to try
param.split = 'IG';     % Currently support 'information gain' only
trees = growTrees(data_train,param);
%{
test_point = [-.5 -.7; .4 .3; -.7 .4; .5 -.5];
for n=1:4
    %retrieves indexes of leaves that test_point ends at
    leaves = testTrees(test_point(n,:),trees);
    %leaves are 10 tree outputs for each test point
    %=tree(T).leaf(leaf_idx).label= cntTotal = index in tree(1).prob(cnt_total,:) = tree(T).node(n).prob';
    
    % average the class distributions of leaf nodes of all trees
    p_rf = trees(1).prob(leaves,:); %10 trees in forest, 10 leaf distributions.
    p_rf_sum = sum(p_rf)/length(trees);
    figure;
    title(sprintf('Class distribution of dest leaf in each decision tree & avg, datapoints %A.',test_point(n,:)));
    for L= 1: size(p_rf,1) %length(tree.leaf)
        subplot(4,3,L);
        bar(p_rf(L));
        axis([0.5 3.5 0 1]);
        title(sprintf('Leaf %i',L));
    end
    subplot(4,3,(size(p_rf,1)+1))
    bar(p_rf_sum);
    title(sprintf('Average'));
end
%}
% Test on the dense 2D grid data, and visualise the results ... 

    
    
    % average the class distributions of leaf nodes of all trees
    %data_test length by 10 trees, average, to data_test length *3
    %select largest class, then plot
    for m=1:size(data_test,1)
        leaves = testTrees(data_test(m,1:2),trees);
        p_rf = trees(1).prob(leaves,:); %10 trees in forest, 10 leaf distributions.
        p_rf_sum = sum(p_rf)/length(trees);
        [val, idx] = max(p_rf_sum);
        data_test(m,3)=idx;
    end
    plot_toydata(data_test);
    





% Change the RF parameter values and evaluate ...


%for i=1: 2^param.depth-1
    %if is leaf node, not 0 data, but no split
%    if ~tree(1).node(i).dim && ~isempty(tree(T).node(n).idx)
        
%    end
%end
%{
%%%%%%%%%%%%%%%%%%%%%
% Evaluate/Test Random Forest

% grab the few data points and evaluate them one by one by the leant RF
test_point = [-.5 -.7; .4 .3; -.7 .4; .5 -.5];
for n=1:4
    leaves = testTrees([test_point(n,:) 0],trees);
    % average the class distributions of leaf nodes of all trees
    p_rf = trees(1).prob(leaves,:);
    p_rf_sum = sum(p_rf)/length(trees);
end


% Test on the dense 2D grid data, and visualise the results ... 

% Change the RF parameter values and evaluate ... 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% experiment with Caltech101 dataset for image categorisation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

init;

% Select dataset
% we do bag-of-words technique to convert images to vectors (histogram of codewords)
% Set 'showImg' in getData.m to 0 to stop displaying training and testing images and their feature vectors
[data_train, data_test] = getData('Caltech');
close all;



% Set the random forest parameters ...
% Train Random Forest ...
% Evaluate/Test Random Forest ...
% show accuracy and confusion matrix ...


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% random forest codebook for Caltech101 image categorisation
% .....
%}



